{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Финальное задание:\n",
    "# - chunking function for 3 types of n-grams (к примеру не + какая-то часть речи или NP или сущ.+ наречие и тд)\n",
    "# - 3 шаблона для записи в словарь в прошлую дз (такие, как в первом пункте)\n",
    "# ##  Пояснение по баллам:\n",
    "# Балл за объяснение того, почему именно эти группы вы взяли, балл за создание такого рода чанкера, балл за за встраивание функции в программу из предыдущей домашки, балл за сравнение качества предсказания тональности с улучшением и без."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "import pandas as pd\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "#import conllu\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pymorphy2\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers as layer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import utils\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split   \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting stanza\n",
      "  Downloading stanza-1.5.1-py3-none-any.whl (865 kB)\n",
      "     -------------------------------------- 865.2/865.2 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "     ------------------------------------- 358.9/358.9 kB 21.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (1.25.1)\n",
      "Collecting protobuf>=3.15.0\n",
      "  Downloading protobuf-4.24.3-cp310-abi3-win_amd64.whl (430 kB)\n",
      "     ------------------------------------- 430.5/430.5 kB 26.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (2.26.0)\n",
      "Collecting torch>=1.3.0\n",
      "  Downloading torch-2.0.1-cp311-cp311-win_amd64.whl (172.3 MB)\n",
      "     ------------------------------------- 172.3/172.3 MB 15.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stanza) (4.66.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (4.7.1)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "     ---------------------------------------- 5.7/5.7 MB 28.1 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 43.9 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ---------------------------------------- 133.1/133.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.3-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 536.2/536.2 kB ? eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, protobuf, networkx, MarkupSafe, emoji, jinja2, torch, stanza\n",
      "Successfully installed MarkupSafe-2.1.3 emoji-2.8.0 jinja2-3.1.2 mpmath-1.3.0 networkx-3.1 protobuf-4.24.3 stanza-1.5.1 sympy-1.12 torch-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ru-core-news-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.5.0/ru_core_news_md-3.5.0-py3-none-any.whl (41.9 MB)\n",
      "     ---------------------------------------- 41.9/41.9 MB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ru-core-news-md==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: pymorphy3>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ru-core-news-md==3.5.0) (1.2.0)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.5.0) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.5.0) (0.6.2)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.5.0) (2.4.417150.4580142)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (1.10.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (6.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (1.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (58.1.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (2.0.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-md==3.5.0) (2.1.1)\n",
      "Installing collected packages: ru-core-news-md\n",
      "Successfully installed ru-core-news-md-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: docopt is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559"
     ]
    }
   ],
   "source": [
    "! pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "my = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание текста, выбранного для анализа\n",
    "Для анализа была взята поэма \"Повесть каменного века\". В тексте много примеров, сложных для анализа. Примеры:\n",
    "- Омонимы (неполные омографы): \"зари\" - в значении ед.ч. род.п. и мн.ч. им.п.\n",
    "- Причастия/прилагательные: \"преданной\" (преданной анафеме/преданной любимым)\n",
    "- Сложные слова с дефисным написанием: \"рок-судья\"\n",
    "- Окказионализмы (выдуманные слова): \"шестопером\" - и другие слова, которые могут не содержаться в \"обучающей\" выборке парсера (архаизмы - \"ея\")\n",
    "## Какой тегсет использовать?\n",
    "В качестве тегсета был использован тэгсетов Universal Dependencies - один из самых широко используемых среди всех тегсетов. Во-первых, его примуществом является тот факт, что он используется во многих библиотеках. Во-вторых, некоторые аспекты этого тэгсета помогают обнаружить проблемы разметки (в том числе другими тэгсетами). Например, он разделяет союзы, используемые в сложносочиненных предложениях (CCONJ) и сложноподчиненных предложениях (SCONJ). Даже тэгстет mystem, который разработан специально для русского языка, не обладает такими хараткеристиками. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r'C:\\Users\\User\\PycharmProjects\\practice\\dep2label\\«Где И.txt', 'r', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spis = []\n",
    "with open(r'C:\\Users\\User\\PycharmProjects\\practice\\dep2label\\«Где И.txt','r', encoding=\"utf-8\") as fp:\n",
    "    for line in fp:\n",
    "        spis.append(line.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_spis = []\n",
    "for i in spis:\n",
    "    token_spis.append(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "df = pd.DataFrame(columns = {'tokens'})\n",
    "for spis in token_spis:\n",
    "    for el in spis:\n",
    "        if el not in list(string.punctuation):\n",
    "            tokens.append(el)\n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.to_excel('corpus.xlsx', encoding = 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_stem = my.analyze(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_data = pd.DataFrame()\n",
    "stem_tags = []\n",
    "stem_words = []\n",
    "for w in doc_stem:\n",
    "    if 'analysis' in w.keys() and len(w['analysis'])!=0:\n",
    "        stem_tags.append(re.split(\"=|,| \", w['analysis'][0]['gr'])[0])\n",
    "        stem_words.append(str(w['text']).lower())\n",
    "stem_data['tokens'] = stem_words\n",
    "stem_data['tags'] = stem_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>s_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>где</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>и</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>в</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>лесу</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>дремучем</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens  s_tags\n",
       "0       где  ADVPRO\n",
       "1         и       S\n",
       "2         в      PR\n",
       "3      лесу       S\n",
       "4  дремучем       A"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_data = stem_data.rename(columns={'tags': 's_tags'})\n",
    "stem_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_data = pd.DataFrame()\n",
    "morph_tags = []\n",
    "morph_words = []\n",
    "for word in tokens:\n",
    "   morph_tags.append(m.parse(word)[0].tag), morph_words.append(m.parse(word)[0].word)\n",
    "morph_data['tags']=morph_tags\n",
    "morph_data['tokens']=morph_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PNCT</td>\n",
       "      <td>«</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADVB,Ques</td>\n",
       "      <td>где</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CONJ</td>\n",
       "      <td>и</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PREP</td>\n",
       "      <td>в</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOUN,inan,masc sing,loc2</td>\n",
       "      <td>лесу</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       tags tokens\n",
       "0                      PNCT      «\n",
       "1                 ADVB,Ques    где\n",
       "2                      CONJ      и\n",
       "3                      PREP      в\n",
       "4  NOUN,inan,masc sing,loc2   лесу"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_pos(row):\n",
    "    if ',' in str(row['tags']):\n",
    "        return str(row['tags']).split(',')[0]\n",
    "    else:\n",
    "        return str(row['tags'])\n",
    "morph_data['tags'] = morph_data.apply(only_pos, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PNCT</td>\n",
       "      <td>«</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADVB</td>\n",
       "      <td>где</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CONJ</td>\n",
       "      <td>и</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PREP</td>\n",
       "      <td>в</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>лесу</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  m_tags tokens\n",
       "0   PNCT      «\n",
       "1   ADVB    где\n",
       "2   CONJ      и\n",
       "3   PREP      в\n",
       "4   NOUN   лесу"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_data = morph_data.rename(columns={'tags': 'm_tags'})\n",
    "morph_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_data = pd.DataFrame()\n",
    "sp_tokens = []\n",
    "sp_tags = []\n",
    "for token in document:\n",
    "    sp_tokens.append(str(token).lower())\n",
    "    sp_tags.append(token.pos_)\n",
    "spacy_data['tokens'] = sp_tokens\n",
    "spacy_data['tags_sp'] = sp_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags_sp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>«</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>и</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>?</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tokens tags_sp\n",
       "0      «   PUNCT\n",
       "1    где     ADV\n",
       "2      и    PART\n",
       "3      ?   PUNCT\n",
       "4     \\n   SPACE"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_data = spacy_data[spacy_data['tags_sp']!='PUNCT']\n",
    "morph_data = morph_data[morph_data['m_tags']!='PNCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_data = spacy_data[spacy_data['tags_sp']!='SPACE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_data.to_excel('morph.xlsx')\n",
    "stem_data.to_excel('stem.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_data.to_excel('spacy.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'C:\\Users\\User\\PycharmProjects\\practice\\dep2label\\corpus.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tokens</th>\n",
       "      <th>POS</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>и</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>в</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>лесу</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>дремучем</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>230</td>\n",
       "      <td>ей</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>231</td>\n",
       "      <td>в</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>232</td>\n",
       "      <td>делах</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>233</td>\n",
       "      <td>ее</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>234</td>\n",
       "      <td>погонь</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    tokens    POS Unnamed: 3\n",
       "0             1       где    ADV        NaN\n",
       "1             2         и  CCONJ        NaN\n",
       "2             3         в    ADP        NaN\n",
       "3             4      лесу   NOUN        NaN\n",
       "4             5  дремучем    ADJ        NaN\n",
       "..          ...       ...    ...        ...\n",
       "229         230        ей   PRON        NaN\n",
       "230         231         в    ADP        NaN\n",
       "231         232     делах   NOUN        NaN\n",
       "232         233        ее   PRON        NaN\n",
       "233         234    погонь   NOUN        NaN\n",
       "\n",
       "[234 rows x 4 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['tokens'].map(lambda x: str(x).lower())\n",
    "#data = data.drop(['Unnamed: 0', 'Unnamed: 3'], axis= 1 , inplace= True )\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0', 'Unnamed: 3'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged =pd.merge(data,spacy_data, on = 'tokens')\n",
    "data_merged= data_merged.merge(morph_data, on = 'tokens')\n",
    "data_merged = data_merged.merge(stem_data, on = 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>POS</th>\n",
       "      <th>tags_sp</th>\n",
       "      <th>m_tags</th>\n",
       "      <th>s_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokens  POS tags_sp m_tags  s_tags\n",
       "0     где  ADV     ADV   ADVB  ADVPRO\n",
       "1     где  ADV     ADV   ADVB  ADVPRO\n",
       "2     где  ADV     ADV   ADVB  ADVPRO\n",
       "3     где  ADV     ADV   ADVB  ADVPRO\n",
       "4     где  ADV     ADV   ADVB  ADVPRO\n",
       "5     где  ADV     ADV   ADVB  ADVPRO\n",
       "6     где  ADV     ADV   ADVB  ADVPRO\n",
       "7     где  ADV     ADV   ADVB  ADVPRO\n",
       "8     где  ADV     ADV   ADVB  ADVPRO\n",
       "9     где  ADV     ADV   ADVB  ADVPRO\n",
       "10    где  ADV     ADV   ADVB  ADVPRO\n",
       "11    где  ADV     ADV   ADVB  ADVPRO\n",
       "12    где  ADV     ADV   ADVB  ADVPRO\n",
       "13    где  ADV     ADV   ADVB  ADVPRO\n",
       "14    где  ADV     ADV   ADVB  ADVPRO\n",
       "15    где  ADV     ADV   ADVB  ADVPRO\n",
       "16    где  ADV     ADV   ADVB  ADVPRO\n",
       "17    где  ADV     ADV   ADVB  ADVPRO\n",
       "18    где  ADV     ADV   ADVB  ADVPRO\n",
       "19    где  ADV     ADV   ADVB  ADVPRO"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>tags_sp</th>\n",
       "      <th>m_tags</th>\n",
       "      <th>s_tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>где</th>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "      <td>ADVPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>и</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>PART</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>и</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>PART</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>CONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>и</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>и</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>CONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>длиною</th>\n",
       "      <td>CCONJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>многое</th>\n",
       "      <td>PRON</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NPRO</td>\n",
       "      <td>SPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>могуч</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ADJS</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>прост</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADJS</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>мык</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>UNKN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          POS tags_sp m_tags  s_tags\n",
       "tokens                              \n",
       "где       ADV     ADV   ADVB  ADVPRO\n",
       "и       CCONJ    PART   CONJ       S\n",
       "и       CCONJ    PART   CONJ    CONJ\n",
       "и       CCONJ   CCONJ   CONJ       S\n",
       "и       CCONJ   CCONJ   CONJ    CONJ\n",
       "...       ...     ...    ...     ...\n",
       "длиною  CCONJ    NOUN   NOUN       S\n",
       "многое   PRON    NOUN   NPRO    SPRO\n",
       "могуч     ADJ   PROPN   ADJS       A\n",
       "прост     ADJ    NOUN   ADJS       A\n",
       "мык      NOUN   PROPN   UNKN       S\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.set_index('tokens').drop_duplicates()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_dict = {'A':'ADJ','ADVPRO':'ADV','ANUM':'ADJ','APRO':'DET','COM':'ADJ','CONJ':'SCONJ','NONLEX':'X','PR':'ADP',\n",
    "             'S':'NOUN','SPRO':'PRON','UNKN':'X','V':'VERB'}\n",
    "morph_dict = {'ADVB':'ADV','CONJ':'CCONJ','PREP':'ADP','ADJF':'ADJ',\n",
    "              'NRPO':'PRON','PRCL':'PART','GRND':'VERB'\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\User\\PycharmProjects\\practice\\dep2label\\corpus.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0',  axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>POS</th>\n",
       "      <th>tokens.1</th>\n",
       "      <th>m_tags</th>\n",
       "      <th>tokens.2</th>\n",
       "      <th>tags_sp</th>\n",
       "      <th>tokens.3</th>\n",
       "      <th>s_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "      <td>где</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>И</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>и</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>и</td>\n",
       "      <td>PART</td>\n",
       "      <td>и</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>В</td>\n",
       "      <td>ADP</td>\n",
       "      <td>в</td>\n",
       "      <td>ADP</td>\n",
       "      <td>в</td>\n",
       "      <td>ADP</td>\n",
       "      <td>в</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>дремучем</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>дремучем</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>дремучем</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>дремучем</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Свои</td>\n",
       "      <td>PRON</td>\n",
       "      <td>свои</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>свои</td>\n",
       "      <td>DET</td>\n",
       "      <td>свои</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tokens    POS  tokens.1 m_tags  tokens.2 tags_sp  tokens.3 s_tags\n",
       "0       Где    ADV       где    ADV       где     ADV       где    ADV\n",
       "1         И  CCONJ         и  CCONJ         и    PART         и   NOUN\n",
       "2         В    ADP         в    ADP         в     ADP         в    ADP\n",
       "4  дремучем    ADJ  дремучем    ADJ  дремучем     ADJ  дремучем    ADJ\n",
       "8      Свои   PRON      свои    ADJ      свои     DET      свои    DET"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\n",
    "      .query(\"m_tags in @morph_dict.keys()\")\n",
    "      .assign(m_tags=lambda x: x[\"m_tags\"].replace(morph_dict)))\n",
    "df = (df\n",
    "      .query(\"s_tags in @stem_dict.keys()\")\n",
    "      .assign(s_tags=lambda x: x[\"s_tags\"].replace(stem_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(stand, comp):\n",
    "    counter = 0 \n",
    "    for s, c in zip(stand,comp):\n",
    "        if s == c:\n",
    "            counter+=1\n",
    "        # else:\n",
    "        #     print(s,c)\n",
    "    return round(counter/len(stand),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MorphAnalyzer: 0.8514\n",
      "Accuracy for Mystem: 0.7838\n",
      "Accuracy for Spacy: 0.8378\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for MorphAnalyzer:',comparison(list(df['POS'].values),list(df['m_tags'].values)))\n",
    "print('Accuracy for Mystem:',comparison(list(df['POS'].values),list(df['s_tags'].values)))\n",
    "print('Accuracy for Spacy:',comparison(list(df['POS'].values),list(df['tags_sp'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Находим шаблоны для chunking\n",
    "Чтобы понять, какие n-gramm надо доставать из текстов, чтобы определить их тональность, рассмотрим биграммы, которые встречаются в текстах с уже определенной положительной и отрицательной тональностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.session()\n",
    "ua = UserAgent(verify_ssl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# информация из страницы со списком новостей\n",
    "def parse_news_page_block(one_block):\n",
    "    block = {}\n",
    "    ua = UserAgent(verify_ssl=False)\n",
    "    a = one_block.find('a')\n",
    "    block['href'] = a.attrs['href']\n",
    "    return block\n",
    "# отдельная страница новости   \n",
    "def parse_one_article(block):\n",
    "    ua = UserAgent(verify_ssl=False)\n",
    "    url_one = 'https://spasibovsem.ru' + block['href']\n",
    "    block['page_id'] = block['href']\n",
    "    req = session.get(url_one, headers={'User-Agent': ua.random})\n",
    "    time.sleep(2)\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    block['full_text'] = soup.find(\"div\", {\"class\": \"text response-text description\"}, {\"itemprop\": \"reviewBody\"}).text   \n",
    "    block['rating'] = (str(soup.find(\"abbr\", {\"class\": \"rating\"}))).split('>')[0].split()[2].split('\"')[1]\n",
    "    return block \n",
    "def get_nth_page(page_number):\n",
    "    # скачиваем\n",
    "    url =  f'https://spasibovsem.ru/filmy-otzyvy/?page={page_number}'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    time.sleep(2)\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # находим новости\n",
    "    news = soup.find_all('div', {'class': 'response-name'})\n",
    "    \n",
    "    # идем по новостям и обрабатываем их\n",
    "    blocks = []\n",
    "    for n in news:\n",
    "        try:\n",
    "            blocks.append(parse_news_page_block(n))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    seen_news = []\n",
    "    # идем по отдельным статьям и достаем информацию\n",
    "    result = []\n",
    "    for b in blocks:\n",
    "        try:\n",
    "            res = parse_one_article(b)\n",
    "            res['id'] = b['page_id']\n",
    "            result.append(res)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    # возвращаем найденную информацию\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(n_pages):\n",
    "    reses = []\n",
    "    for i in tqdm(range(n_pages)):\n",
    "        blocks = get_nth_page(i+1)\n",
    "        reses.append(blocks)\n",
    "    return reses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [14:21<38:43, 105.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [15:57<35:53, 102.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n",
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [17:30<33:12, 99.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [27:32<24:18, 104.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [30:16<26:28, 122.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [32:58<26:50, 134.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [51:54<13:16, 159.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [57:14<07:59, 159.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [59:52<05:18, 159.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:05:03<00:00, 130.12s/it]\n"
     ]
    }
   ],
   "source": [
    "spis = run_all(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = pd.DataFrame(columns =['texts', 'rating', 'tonality'])\n",
    "texts = []\n",
    "ratings = []\n",
    "for page in spis:\n",
    "    for slovik in page:\n",
    "        texts.append(slovik['full_text'])\n",
    "        ratings.append(slovik['rating'])\n",
    "rev['texts'] = texts\n",
    "rev['rating'] = ratings\n",
    "tone_dictionary ={'1' : 0, '2' : 0, '4' : 1, '5' : 1, '3' : 0.5}\n",
    "#df[df['rating']=='3']['tonality'] = 0.5\n",
    "rev['tonality'] = rev['rating'].map(tone_dictionary)\n",
    "rev = rev.loc[rev['tonality'] != 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_preprocessed = []\n",
    "for a_text in list(rev['texts'].values):\n",
    "    a_tokens = wordpunct_tokenize(a_text)\n",
    "    a_lemmatized = \" \".join([m.parse(item)[0].normal_form.lower() for item in a_tokens])\n",
    "    reviews_preprocessed.append(a_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "slovik_pos = defaultdict(int)\n",
    "slovik_neg = defaultdict(int)\n",
    "for text, tone in zip(reviews_preprocessed, list(rev['tonality'].values)):\n",
    "    if tone == 0:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        bigrams = ngrams(tokens, n, pad_left=True, pad_right=True)\n",
    "        for bigram in bigrams:\n",
    "            slovik_neg[bigram] += 1\n",
    "    if tone == 1:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        bigrams = ngrams(tokens, n, pad_left=True, pad_right=True)\n",
    "        for bigram in bigrams:\n",
    "            slovik_pos[bigram] += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sloviki = [slovik_neg, slovik_pos]\n",
    "for slovik in sloviki:\n",
    "    for k, v in slovik.copy().items():\n",
    "        if v<=2:\n",
    "            del slovik[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pos = defaultdict(int)\n",
    "for el,c in slovik_pos.items():\n",
    "    if el[0] not in list(string.punctuation) and el[1] not in list(string.punctuation):\n",
    "        sorted_pos[el]=c\n",
    "sorted_pos = sorted(sorted_pos.items(), key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_neg = defaultdict(int)\n",
    "for el,c in slovik_neg.items():\n",
    "    if type(el) is not int and el[0] not in list(string.punctuation) and el[1] not in list(string.punctuation):\n",
    "        sorted_neg[el]=c\n",
    "sorted_neg = sorted(sorted_neg.items(), key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор н-грамм\n",
    "В прошлый раз большинство ошибок касались разметки негативных отзывов (из-за несбалансированной выборки), поэтому шаблоны мы будем извлекать для классификации (пополнения словаря) негативных отзывов. \n",
    "\n",
    "Можно заметить, что чаще всего в биграммах встречается частица \"не\", особенно частотными являются биграммы с \"не\"+глагол. Также часто встречаются биграммы союз + \"не\". Кажется, что негативные отзывы должны содержать прилагательное \"плохой\", поэтому также используем биграмму \"плохой\" + Noun (т.к. чаще всего прилагательные сочетаются с существительными, и мы предполагаем, что между ними не будет расстояния,т.е. других словоформ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(text):\n",
    "    text_sp = text.split()\n",
    "    counter = 0\n",
    "    for i in range(len(text_sp)):\n",
    "        try:\n",
    "            if text_sp[i] == 'не' and text_sp[i+1] not in list(string.punctuation) and str(m.parse(text_sp[i+1])[0].tag)=='VERB':\n",
    "                counter+=1\n",
    "            if text_sp[i] not in list(string.punctuation) and str(m.parse(text_sp[i])[0].tag)=='CONJ' and text_sp[i+1] == 'не':\n",
    "                counter+=1\n",
    "            if text_sp[i]=='плохой' and text_sp[i+1] not in list(string.punctuation) and str(m.parse(text_sp[i+1])[0].tag)=='NOUN':\n",
    "                counter+=1\n",
    "        except Exception:\n",
    "            pass\n",
    "    return counter\n",
    "#m.parse(word)[0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-gramms\n",
    "one_pos = {}\n",
    "one_neg = {}\n",
    "for text, tone in zip(reviews_preprocessed, list(rev['tonality'].values)):\n",
    "    if tone == 0:\n",
    "        for word in text.split():\n",
    "            if word not in stops and word not in string.punctuation and word not in one_neg:\n",
    "                one_neg[word] = 1\n",
    "            elif word not in stops and word not in string.punctuation and word in one_neg:\n",
    "                one_neg[word] += 1\n",
    "    if tone == 1:\n",
    "        for word in text.split():\n",
    "            if word not in stops and word not in string.punctuation and word not in one_pos:\n",
    "                one_pos[word] = 1\n",
    "            elif word not in stops and word not in string.punctuation and word in one_pos:\n",
    "                one_pos[word] += 1  \n",
    "sloviki = [one_neg, one_pos]\n",
    "for slovik in sloviki:\n",
    "    for k, v in slovik.copy().items():\n",
    "        if v<=2:\n",
    "            del slovik[k]\n",
    "one_neg_set = []\n",
    "one_pos_set = []\n",
    "for k in one_neg.keys():\n",
    "    if k not in list(one_pos.keys()):\n",
    "        one_neg_set.append(k)\n",
    "for k in one_pos.keys():\n",
    "    if k not in list(one_neg.keys()):\n",
    "        one_pos_set.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [03:06<21:46, 186.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [06:01<17:59, 179.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [17:37<05:46, 173.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid return character or leading space in header: User-Agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [24:23<00:00, 182.99s/it]\n"
     ]
    }
   ],
   "source": [
    "def run_test(n_pages):\n",
    "    reses = []\n",
    "    for i in tqdm(range(n_pages)):\n",
    "        blocks = get_nth_page(i+100)\n",
    "        reses.append(blocks)\n",
    "    return reses\n",
    "test_spis = run_test(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(columns =['texts', 'rating', 'tonality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus = [test_spis]\n",
    "texts = []\n",
    "ratings = []\n",
    "for spis in clus:\n",
    "    for page in spis:\n",
    "        for slovik in page:\n",
    "            texts.append(slovik['full_text'])\n",
    "            ratings.append(slovik['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['texts'] = texts\n",
    "test['rating'] = ratings\n",
    "test['tonality'] = test['rating'].map(tone_dictionary)\n",
    "test = test.loc[test['tonality'] != 0.5]\n",
    "reviews_test_preprocessed = []\n",
    "for a_text in list(test['texts'].values):\n",
    "    a_tokens = wordpunct_tokenize(a_text)\n",
    "    a_lemmatized = \" \".join([m.parse(item)[0].normal_form.lower() for item in a_tokens])\n",
    "    reviews_test_preprocessed.append(a_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "res_tone = []\n",
    "errors = 0\n",
    "for text in reviews_test_preprocessed:\n",
    "    neg = 0\n",
    "    pos = 0\n",
    "    neg+=chunker(text)\n",
    "    for word in text.split():\n",
    "        if word not in stops and word not in string.punctuation and word in one_neg_set:\n",
    "            neg+=1\n",
    "        elif word not in stops and word not in string.punctuation and word in one_pos_set:\n",
    "            pos+=1\n",
    "    if neg>pos:\n",
    "        res_tone.append(0)\n",
    "    elif pos>neg:\n",
    "        res_tone.append(1)\n",
    "    else:\n",
    "        errors+=1\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89\n"
     ]
    }
   ],
   "source": [
    "print(round(sum(1 for x,y in zip(res_tone, test['tonality']) if x == y) / len(res_tone),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "res_one = []\n",
    "errors = 0\n",
    "for text in reviews_test_preprocessed:\n",
    "    neg = 0\n",
    "    pos = 0\n",
    "    for word in text.split():\n",
    "        if word not in stops and word not in string.punctuation and word in one_neg_set:\n",
    "            neg+=1\n",
    "        elif word not in stops and word not in string.punctuation and word in one_pos_set:\n",
    "            pos+=1\n",
    "    if neg>pos:\n",
    "        res_one.append(0)\n",
    "    elif pos>neg:\n",
    "        res_one.append(1)\n",
    "    else:\n",
    "        res_one.append(0.5)\n",
    "        errors+=1\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_without_bi = sum(1 for x,y in zip(res_one, test['tonality']) if x == y) / len(res_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8623076923076923\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_without_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "результат: точность выросла на 3%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
